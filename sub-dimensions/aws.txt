# AWS

## Glue
> https://github.com/TheGaneshChandra/awsglue

## Kinesis

### Kinesis Data stream
> Near real time bits of data transfer from PRODUCER to AWS which a PRODUCER can consume
- Streams
    |
    |-- Shards <!-- Different threads based on partition key of data in a stream -->
        |
        |-- Producer <!-- Any device or application that sends data to Kinesis data streams -->
        |   |
        |   |-- Send One record per call <kinesis.put_record( StreamName="", Data="", PartitionKey="")>
        |   |-- Send Batch upto 500 <kinesis.put_records( StreamName="", Records=[ {"Data":, "PartitionKey":} ])>
        |
        |-- SequenceNumber <!-- A unique number that gets incremented for each record in shard -->
        |
        |-- PartitionKey <!-- A column used to define which shard the data goes to -->
        |   |
        |   |-- MD5_Hashing <!-- A type of hash that gives values in range 0-2^128, Used to differentiate unique partition keys -->
        |
        |-- ShardIterator <!-- A pointer from where the shard data is read forward -->
        |   |
        |   |-- "TRIM HORIZON" <!-- Starts from the oldes record -->
        |   |-- "LATEST" <!-- Starts from the latest -->
        |   |-- "AT_SEQUENCE_NUMBER" <!-- Starts from a given sequence number -->
        |   |-- "AFTER_SEQUENCE_NUMBER" <!-- Starts from the next record of given seq number -->
        |   |-- "AT_TIMESTAMP" <!-- Starts from the given timestamp-->
        |
        |-- Retention_Period <!-- Amount of time Data is alive in data stream before getting deleted (default: 24hours, max: 365days) -->
        |
        |-- Checkpointing <!-- Saves the last sequenceNumber so to process only the next records handled by user or using KCL -->
        |
        |-- Resharding <!-- Splitting or Merging of Shards based on Throughput limit (write: 1MB/sec, 1000 records/sec) or (read: 2 MB/sec, 5 Get_record() calls -->
        |
        |-- Provisioned_Mode <!-- AWS handling the Automatic scaling (Resharding) of shards based on data throughput-->
        |
        |-- Consumers <!-- Any Application that reads data from Kinesis data streams -->
        |   |
        |   |-- Classic <GetRecords(shard_iterator)> <!-- Total of 2MB/sec Bandwidth for all consumers with around 200ms latency -->
        |   |-- Enhanced_Fan_Out (EFO) <SubscribeToShard()> <!-- Each consumer gets 2MB/sec bandwidth with 70ms latency -->
        |
        |-- Kinesis_Client_Library (KCL) <!-- A library that handles some tasks related to Kinsis -->
            |
            |-- Checkpointing <checkpoint(sequence_number)> <!-- Saves the last sequenceNumber so to process only the next records (stored in dynamodb) -->
            |-- Shard_Discovery
            |-- LoadBalancing
            |-- Fault recovery
            |-- ExcatlyOnceProcessing

### Kinesis FireHose
> Fully managed streaming service that **delivers** data to S3, RedShift, OpenSearch, Splunk, Snowflake
- Delivery Stream <!-- The stream that moves the data from producer to the consumer destination -->
    ```python
        
        import boto3
        firehose = boto3.client("firehose")
        firehose.create_delivery_stream(
            DeliveryStreamName="",
            S3DestinationConfiguration = {}
        )

        # send record
        firehose.put_record(
            DeliveryStreamName = "",
            Record = [{}]
        )
    ```
    |
    |-- Buffering <!-- Firehose delivers data in a batch format, so if buffers the stream data based on chosen size(1MB-128MB), interval(1-900sec) -->
    |
    |-- Data Transformtion using Lambda <!-- Lambda function to be executed on each batch before putting into Consumer destination -->
    |
    |-- Compression Formats <!-- Can Compress before writing to destination (GZIP, Snappy, ZIP) -->
    |
    |-- Data Formats <!-- Can convert the data to desired format (JSON, PARQUET, ORC) before writing  -->
    |
    |-- S3 backup mode <!-- if writing to destination fails, will save the data in this backup bucket -->
    |
    |-- Auto Retires
    |
    |-- AutoScaling

### AWS Managed Apache Flink
> A distributed processing engine for stateful computations of bounded and unbounded data
- Stateful Stream processing <!-- Remembers past events, Keep local state on each operator checkpointed to S3/Dynamodb-->
- Event-Time processing <!-- Processes events when happened, not when arrived -->
- Exaclty once Stateful processing Guarenteed <!-- Industry standard for exactly once processing -->
- Windowing functionaliy <!-- events grouped by time or count -->
- Fault Tolerance, Savepointing <!-- Can restart entire pipeline without losing state -->
- Batch + Streaming

### AWS Managed Kafka
> Distributed log-based Event streaming platform used for messaging, **event storage**, and streaming analytics
- Durable storage layer <!-- Can store data upto years, replay history at any time -->
- Stream Replay (Time travel) <!-- Consume old data at any offset by any consumer -->
- Independent Consumers <!-- Consumers can access, replay data independently -->
- Extremely High-throughput <!-- Millions per second -->
- Kafka Connect <!-- can be connected to many connectors with no code -->

## s3

- Buckets
    |
    |-- MetaData tables <!-- Iceberg tables containing Data about the objects in the bucket that can be queryed in Athena-->
    |       |
    |       |-- Journal Metadata <!-- Default Metadata table, records events occured on objects inside bucket -->
    |       |       |
    |       |       |-- SystemDefined <!-- Created automatically by aws, can also manually specify when uploading a new object -->
    |       |       |-- UserDefined <!-- Custom key value pairs given by user -->
    |       |
    |       |-- Inventory Metadata <!-- Metadata table containing list of all objects and version present in the bucket -->
    |
    |-- Properties
    |       |
    |       |-- BucketVersioning <!-- Keeps Versions of objects instead of deleting them permanently -->
    |       |-- MFA delete <!-- Needs an MFA key to delete; This can only be enabled using CLI, SDK -->
    |       |
    |       |-- Bucket AttributeBasedAccessControl ABAC <!-- Uses user defined tags to grant IAM permissions -->
    |       |
    |       |-- Tags
    |       |
    |       |-- Encryption
    |       |       |
    |       |       |-- Server side Encryption with S3 Managed Keys
    |       |       |-- Server side Encryption with KMS
    |       |       |-- Dual Layer Server side Encryption with KMS
    |       |       |-- Server side Encryption with Customer Provided Keys
    |       |       |
    |       |       |-- S3 Bucket Keys <!-- Uses one temporary key for Bucketlevel Encryption, which is used to create dataKey for each object without reaching out to KMS all the time -->
    |       |
    |       |-- S3_Intelligent_Tiering <!-- If data access is not predicatable then S3_Intelligent_Tiering; If data access is predictable tjen s3 LifeCycle Rules -->
    |       |
    |       |-- S3 Server Access Loggging <!-- Logs requests for server access -->
    |       |
    |       |-- s3_Event_Notifications <!-- Create an Event Notification that notifies when that event occurs, used to trigger other stuff -->
    |       |
    |       |-- s3_TransferAccelertion 
    |       |
    |       |-- s3_Object_Lock
    |       |       |
    |       |       |-- Object Lock Legal Hold <!-- Will not allow the object deletion until the object lock is removed by anyone -->
    |       |       |-- Object Lock Retention <!-- Not even root user can delete the object until Retention period expires -->
    |       |
    |       |-- Requestor_pays
    |       |
    |       |-- Static Website Hosting
    |
    |-- Management
    |       |
    |       |-- LifeCycle configuration <!-- deletes / Moves the selected object versions between storage classes based on given condition -->
    |       |
    |       |-- Replication rules <!-- Replicates the selected contents from source bucket to destination bucket -->
    |       |       |
    |       |       |-- Replication Time Control <!-- Replicates within 15 mins -->
    |       |       |-- Delete Marker Replication <!-- Replicates the delete marker also, this is disabled by default -->
    |       |       |-- Replica Modification Sync <!-- Replicates the metadata changes too -->
    |       |
    |       |-- Inventory configuration <!-- Creates reports with list of objects and their metadata -->
    |
    |-- Access Points
    
- Objects
    |
    |-- Checksums <!-- used to check if the data before uploading and after uploading are same using hashing algorithms -->


## DynamoDB
```
# in all dynamodb function calls, we can add these expressions -- these are unique and important
- ConditionalExpression
- UpdateExpression
- ExpressionAttributeNames
- ExpressionAttributeValues
- ProjectionExpression
```
- Tables
    |
    |-- PartitionKey <!-- Primary key for DynamoDB, stores values in this column as hash and used for quick retreival -->
    |-- SortKey <!-- If a sort key is provided, then the PartitionKey & SortKey together create a composite Primary key -->
    |
    |-- Secondary Index
    |       |
    |       |-- Local Secondary Index (LSI) {Creates an Index based on table's parititionKey and new sortkey; used to query based on this new sortkey}
    |       |-- Global Secondary Index (GSI) {Creates an Index based on given parititionKey and sortKey}
    |       |
    |       |-- AttributeProjections {Saves extra Attributes/columns along with PartitionKey, SortKey in the indices}
    |
    |-- Table Class
    |       |
    |       |-- DynamoDB Standard
    |       |-- DynamoDB Standard Infrequent-Access (Standard IA)
    |
    |-- Capacity
    |       |
    |       |-- On-demand (we have an option for maximum table throuput to limit)
    |       |-- Provisioned
    |                |
    |                |-- AutoScaling (1-anyupper limit)
    |                |
    |                |-- RCU (Read Capacity Units) {1 RCU: 4kb/sec reading (eventually consistent - not so latest results); 1 RCU: 2kb/sec reading (strongly consistent - latest results); 1RCU: 1kb/sec reading (transactional)}
    |                |-- WCU (Write Capacity Units) {1 WCU: 1kb/sec (standard); 1 WCU: 0.5kb/sec (transactional)}
    |
    |-- DynamoDB Streams {provides the Change Data Capture (CDC) to the users in form of streams -- Can also be used as trigger for Lambda functions -- similar to kinesis data streams but most stuff managed by AWS}
    |
    |-- Warm throuput (Increases the capcacity before peak events)
    |
    |-- Deletion Protection (Protects the table from getting deleted/dropped)
    |
    |-- TTL Time to Live (Automatically deletes items after given time expires)
    |
    |-- Global Tables (Creates a duplicate table in another region -- uses DynamoDB Streams in the backend)
    |
    |-- Point-in-time-recovery (PITR) {Used to take continous backups upto 35days -- Can be restored with specified data backup}
