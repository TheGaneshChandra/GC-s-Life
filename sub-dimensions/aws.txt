# AWS
> Important Links
- https://aws.amazon.com/documentation-overview/ {High level overview of service features}
- https://docs.aws.amazon.com/cli/latest/ {Command line documentation}

## Glue
> https://github.com/TheGaneshChandra/awsglue

## Kinesis

### Kinesis Data stream
> Near real time bits of data transfer from PRODUCER to AWS which a PRODUCER can consume
- Streams
    |
    |-- Shards <!-- Different threads based on partition key of data in a stream -->
        |
        |-- Producer <!-- Any device or application that sends data to Kinesis data streams -->
        |   |
        |   |-- Send One record per call <kinesis.put_record( StreamName="", Data="", PartitionKey="")>
        |   |-- Send Batch upto 500 <kinesis.put_records( StreamName="", Records=[ {"Data":, "PartitionKey":} ])>
        |
        |-- SequenceNumber <!-- A unique number that gets incremented for each record in shard -->
        |
        |-- PartitionKey <!-- A column used to define which shard the data goes to -->
        |   |
        |   |-- MD5_Hashing <!-- A type of hash that gives values in range 0-2^128, Used to differentiate unique partition keys -->
        |
        |-- ShardIterator <!-- A pointer from where the shard data is read forward -->
        |   |
        |   |-- "TRIM HORIZON" <!-- Starts from the oldes record -->
        |   |-- "LATEST" <!-- Starts from the latest -->
        |   |-- "AT_SEQUENCE_NUMBER" <!-- Starts from a given sequence number -->
        |   |-- "AFTER_SEQUENCE_NUMBER" <!-- Starts from the next record of given seq number -->
        |   |-- "AT_TIMESTAMP" <!-- Starts from the given timestamp-->
        |
        |-- Retention_Period <!-- Amount of time Data is alive in data stream before getting deleted (default: 24hours, max: 365days) -->
        |
        |-- Checkpointing <!-- Saves the last sequenceNumber so to process only the next records handled by user or using KCL -->
        |
        |-- Resharding <!-- Splitting or Merging of Shards based on Throughput limit (write: 1MB/sec, 1000 records/sec) or (read: 2 MB/sec, 5 Get_record() calls -->
        |
        |-- Provisioned_Mode <!-- AWS handling the Automatic scaling (Resharding) of shards based on data throughput-->
        |
        |-- Consumers <!-- Any Application that reads data from Kinesis data streams -->
        |   |
        |   |-- Classic <GetRecords(shard_iterator)> <!-- Total of 2MB/sec Bandwidth for all consumers with around 200ms latency -->
        |   |-- Enhanced_Fan_Out (EFO) <SubscribeToShard()> <!-- Each consumer gets 2MB/sec bandwidth with 70ms latency -->
        |
        |-- Kinesis_Client_Library (KCL) <!-- A library that handles some tasks related to Kinsis -->
            |
            |-- Checkpointing <checkpoint(sequence_number)> <!-- Saves the last sequenceNumber so to process only the next records (stored in dynamodb) -->
            |-- Shard_Discovery
            |-- LoadBalancing
            |-- Fault recovery
            |-- ExcatlyOnceProcessing

### Kinesis FireHose
> Fully managed streaming service that **delivers** data to S3, RedShift, OpenSearch, Splunk, Snowflake
- Delivery Stream <!-- The stream that moves the data from producer to the consumer destination -->
    ```python
        
        import boto3
        firehose = boto3.client("firehose")
        firehose.create_delivery_stream(
            DeliveryStreamName="",
            S3DestinationConfiguration = {}
        )

        # send record
        firehose.put_record(
            DeliveryStreamName = "",
            Record = [{}]
        )
    ```
    |
    |-- Buffering <!-- Firehose delivers data in a batch format, so if buffers the stream data based on chosen size(1MB-128MB), interval(1-900sec) -->
    |
    |-- Data Transformtion using Lambda <!-- Lambda function to be executed on each batch before putting into Consumer destination -->
    |
    |-- Compression Formats <!-- Can Compress before writing to destination (GZIP, Snappy, ZIP) -->
    |
    |-- Data Formats <!-- Can convert the data to desired format (JSON, PARQUET, ORC) before writing  -->
    |
    |-- S3 backup mode <!-- if writing to destination fails, will save the data in this backup bucket -->
    |
    |-- Auto Retires
    |
    |-- AutoScaling

### AWS Managed Apache Flink
> A distributed processing engine for stateful computations of bounded and unbounded data
- Stateful Stream processing <!-- Remembers past events, Keep local state on each operator checkpointed to S3/Dynamodb-->
- Event-Time processing <!-- Processes events when happened, not when arrived -->
- Exaclty once Stateful processing Guarenteed <!-- Industry standard for exactly once processing -->
- Windowing functionaliy <!-- events grouped by time or count -->
- Fault Tolerance, Savepointing <!-- Can restart entire pipeline without losing state -->
- Batch + Streaming

### AWS Managed Kafka
> Distributed log-based Event streaming platform used for messaging, **event storage**, and streaming analytics
- Durable storage layer <!-- Can store data upto years, replay history at any time -->
- Stream Replay (Time travel) <!-- Consume old data at any offset by any consumer -->
- Independent Consumers <!-- Consumers can access, replay data independently -->
- Extremely High-throughput <!-- Millions per second -->
- Kafka Connect <!-- can be connected to many connectors with no code -->

## s3

- Buckets
    |
    |-- MetaData tables <!-- Iceberg tables containing Data about the objects in the bucket that can be queryed in Athena-->
    |       |
    |       |-- Journal Metadata <!-- Default Metadata table, records events occured on objects inside bucket -->
    |       |       |
    |       |       |-- SystemDefined <!-- Created automatically by aws, can also manually specify when uploading a new object -->
    |       |       |-- UserDefined <!-- Custom key value pairs given by user -->
    |       |
    |       |-- Inventory Metadata <!-- Metadata table containing list of all objects and version present in the bucket -->
    |
    |-- Properties
    |       |
    |       |-- BucketVersioning <!-- Keeps Versions of objects instead of deleting them permanently -->
    |       |-- MFA delete <!-- Needs an MFA key to delete--- This can only be enabled using CLI, SDK -->
    |       |
    |       |-- Bucket AttributeBasedAccessControl ABAC <!-- Uses user defined tags to grant IAM permissions -->
    |       |
    |       |-- Tags
    |       |
    |       |-- Encryption
    |       |       |
    |       |       |-- Server side Encryption with S3 Managed Keys
    |       |       |-- Server side Encryption with KMS
    |       |       |-- Dual Layer Server side Encryption with KMS
    |       |       |-- Server side Encryption with Customer Provided Keys
    |       |       |
    |       |       |-- S3 Bucket Keys <!-- Uses one temporary key for Bucketlevel Encryption, which is used to create dataKey for each object without reaching out to KMS all the time -->
    |       |
    |       |-- S3_Intelligent_Tiering <!-- If data access is not predicatable then S3_Intelligent_Tiering--- If data access is predictable tjen s3 LifeCycle Rules -->
    |       |
    |       |-- S3 Server Access Loggging <!-- Logs requests for server access -->
    |       |
    |       |-- s3_Event_Notifications <!-- Create an Event Notification that notifies when that event occurs, used to trigger other stuff -->
    |       |
    |       |-- s3_TransferAccelertion 
    |       |
    |       |-- s3_Object_Lock
    |       |       |
    |       |       |-- Object Lock Legal Hold <!-- Will not allow the object deletion until the object lock is removed by anyone -->
    |       |       |-- Object Lock Retention <!-- Not even root user can delete the object until Retention period expires -->
    |       |
    |       |-- Requestor_pays
    |       |
    |       |-- Static Website Hosting
    |
    |-- Management
    |       |
    |       |-- LifeCycle configuration <!-- deletes / Moves the selected object versions between storage classes based on given condition -->
    |       |
    |       |-- Replication rules <!-- Replicates the selected contents from source bucket to destination bucket -->
    |       |       |
    |       |       |-- Replication Time Control <!-- Replicates within 15 mins -->
    |       |       |-- Delete Marker Replication <!-- Replicates the delete marker also, this is disabled by default -->
    |       |       |-- Replica Modification Sync <!-- Replicates the metadata changes too -->
    |       |
    |       |-- Inventory configuration <!-- Creates reports with list of objects and their metadata -->
    |
    |-- Access Points
    
- Objects
    |
    |-- Checksums <!-- used to check if the data before uploading and after uploading are same using hashing algorithms -->


## DynamoDB
```
# in all dynamodb function calls, we can add these expressions -- these are unique and important
- ConditionalExpression
- UpdateExpression
- ExpressionAttributeNames
- ExpressionAttributeValues
- ProjectionExpression
```
- Tables
    |
    |-- PartitionKey <!-- Primary key for DynamoDB, stores values in this column as hash and used for quick retreival -->
    |-- SortKey <!-- If a sort key is provided, then the PartitionKey & SortKey together create a composite Primary key -->
    |
    |-- Secondary Index
    |       |
    |       |-- Local Secondary Index (LSI) {Creates an Index based on table's parititionKey and new sortkey--- used to query based on this new sortkey}
    |       |-- Global Secondary Index (GSI) {Creates an Index based on given parititionKey and sortKey}
    |       |
    |       |-- AttributeProjections {Saves extra Attributes/columns along with PartitionKey, SortKey in the indices}
    |
    |-- Table Class
    |       |
    |       |-- DynamoDB Standard
    |       |-- DynamoDB Standard Infrequent-Access (Standard IA)
    |
    |-- Capacity
    |       |
    |       |-- On-demand (we have an option for maximum table throuput to limit)
    |       |-- Provisioned
    |                |
    |                |-- AutoScaling (1-anyupper limit)
    |                |
    |                |-- RCU (Read Capacity Units) {1 RCU: 4kb/sec reading (eventually consistent - not so latest results)--- 1 RCU: 2kb/sec reading (strongly consistent - latest results)--- 1RCU: 1kb/sec reading (transactional)}
    |                |-- WCU (Write Capacity Units) {1 WCU: 1kb/sec (standard)--- 1 WCU: 0.5kb/sec (transactional)}
    |
    |-- DynamoDB Streams {provides the Change Data Capture (CDC) to the users in form of streams -- Can also be used as trigger for Lambda functions -- similar to kinesis data streams but most stuff managed by AWS}
    |
    |-- Warm throuput (Increases the capcacity before peak events)
    |
    |-- Deletion Protection (Protects the table from getting deleted/dropped)
    |
    |-- TTL Time to Live (Automatically deletes items after given time expires)
    |
    |-- Global Tables (Creates a duplicate table in another region -- uses DynamoDB Streams in the backend)
    |
    |-- Point-in-time-recovery (PITR) {Used to take continous backups upto 35days -- Can be restored with specified data backup}


Redshift
    |
    |-- Clusters {Collection of nodes}
    |       |
    |       |-- Nodes {Consider like a server}
    |       |       |
    |       |       |-- Types
    |       |       |       |
    |       |       |       |-- RA3 Redshift Analytics 3 {New--- Uses RMS--- Decople Compute, Storage--- Multi-AZ as it can just get data from s3}
    |       |       |       |
    |       |       |       |-- DC2 Dense Compute 2 {Old--- Uses local SSD--- Sinlge AZ--- Compute, Storage are hardware inside the node--- Recommended for data < 1TB}
    |       |       |
    |       |       |-- Architechture
    |       |               |
    |       |               |-- Leader Node --> [Compute Node, Compute Node,...]
    |       |               |
    |       |               |-- Node Slices {A Core of the Node's CPU, and the RAM, other stuff also gets splitted}
    |       |               |
    |       |               |-- CPU {Here is where the computataion happens, using the L1 cache, RAM, to perform joins etc}
    |       |               |-- RAM {Used for quick data read}
    |       |               |-- SSD storage {Used for data storage and used in cases when RAM is insufficient}
    |       |               |-- NetworkFabric {The Network layer that moves the data to and from outside}
    |       |
    |       |-- SnapShots {Can instantly copy the entire RMS Data, Metadata to recreate the cluster and store in s3} <Need to know Snapshots != backups>
    |       |       |
    |       |       |-- Incremental Snapshots {Captures only changed blocks in RMS}
    |       |       |-- Automated Snapshots {1 day retention (default) --- Default - Sanpshots taken every 8 hours / 5GBb of data change}
    |       |       |-- Manual Snapshots {Infinite retention by default}
    |       |
    |       |-- Resizing
    |       |       |
    |       |       |-- Elastic Resize {Fast --- Change number of nodes without restart --- Cant change node types --- Can only do minor changes --- Nodes have to be same family}
    |       |       |
    |       |       |-- Classic Resize {Slow --- Change number of nodes, node types too --- Can change node types --- Can do drastic node changes --- Nodes can be any family}
    |       |
    |       |-- Distribution {Distributing / Partitioning Data accross nodes}
    |               |
    |               |-- ALL Distribution {Distribute entire table accross nodes}
    |               |-- KEY Distribution {Distribute based on column}
    |               |-- EVEN Distribution {Distribute data evenly based on rows}
    |               |-- AUTO Distribtion {Redshift chooses the Distribution style in the order (ALL -> KEY -> EVEN)}
    |
    |        
    |-- RMS Redshift Managed Storage {Data for the warehouse is stored --- Stores Block Level data in S3 --- Only used in RA3 nodes --- On the backend its either S3 for cold data or SSD of the node for hot data}
    |       |
    |       |-- Sharing data accross AWS Regions {Works only with RA3 --- Wont copy data, just sends the related data to other regions SSD from the current region's S3}
    |
    |-- Commands
    |       |
    |       |-- `VACCUM [type_of_vaccum] [[table_name | TO threshold_pct] [BOOST]]` (Used to reclaim space by resorting the table on which detles and updates might have occuerd)
    |       |       |
    |       |       |-- FULL (Sorts and reclaims space)
    |       |       |-- SORT ONLY (Just sorts the table but wont reclaim any space)
    |       |       |-- DELETE ONLY (Recalim disk space only, wont sort anything)
    |       |       |-- REINDEX (Rebuilds the index of the tables)
    |       |       |-- RECLUSTER (Same as FULL Vaccum, but better)
    |       |
    |       |-- `COPY table_name from 'location' IAM_ROLE 'iamrole'`  (Used to load large amound of data to redshift --- uses Redshift MPP --- support compression --- uses manifest file for s3 files)
    |       |
    |       |-- `INSERT INTO table select * from table_name` (Inserts data into redshift from catalog table or Redshift spectrum --- Much slower than copy)
    |       |
    |       |-- `UNLOAD (query) TO 's3 uri ending with _' IAM_ROLE 'iamrole' FORMAT AS PARQUET` (Inserts data into s3 from Redshift based on query)
    |       |
    |       |-- `ANALYSE table_name` (Used to decide if we need to vaccum the table)
    |
    |
    |-- Federated Queries (Passes queries to other Independent services like RDS, Aurora for them to compute and copies the result to be used with Redshift data)
    |       ``` 
    |           CREATE EXTERNAL SCHEMA abc_rds
    |           FROM POSTGRES
    |           DATABASE 'admin_db'
    |           SCHEMA 'dbo'
    |           URI 'endpointofRDSDatabase' -- identfies the location of the resource
    |           IAM_ROLE 'arn:aws:iam::accountid:role/role_name' -- gets the current IAM role to access the resource
    |           SECRET_ARN 'arn:aws:secretsmanager::....' -- gets the uname, pass from aws secrets manager
    |       ```
    |       |
    |       |-- Services
    |               |
    |               |-- RDS (only POSTGRES, MYSQL)
    |               |-- s3 (using Redshift spectrum)
    |               |-- Aurora (only POSTGRES, MYSQL)
    |               |-- Kinesis
    |
    |-- Materialized Views (Used to get precomputed resutls based on query)
    |       ```
    |            CREATE MATERIALIZED VIEW mv_name
    |            [BACKUP {YES | NO}] -- if yes will be included in redshift snapshots
    |            [table attributes] -- attributes like distribution key, sort key
    |            [AUTO REFRESH {YES | NO}] -- f
    |            AS query
    |       ```
    |       |
    |       |-- Streaming services (using federated queries schema)
    |
    |-- RedShift Spectrum (A Redshift extension that can scan query s3 tables directly without using redshift cluster computation --- mostly like athena but can be used to join with actual redshift tables --- only for reading cant update, delete, insert --- user should have access to create temporary tables)
    |       ```
    |           CREATE EXTERNAL SCHEMA spectrum_schema
    |           FROM data catalog
    |           DATABASE 'admin_f1'
    |           iam_role 'arn:aws:......'
    |           CREATE EXTERNAL DATABASE IF NOT EXISTS
    |       ```
    |       |
    |       |-- Catalogs
    |               |
    |               |-- Glue data catalog
    |               |-- Apache hive metastore
    |
    |-- System tables & Views [https://docs.aws.amazon.com/redshift/latest/dg/cm_chap_system-tables.html]
    |   (Internal metadata of redshift)
    |       |
    |       |-- SVV metadata views 
    |       |-- SYS monitoring views (present in `pg_catalog` schema)
    |       |-- STL views for logging 
    |       |-- STV tables for snapshot data
    |       |-- SVCS views for main and concurrency scaling Clusters
    |       |-- SVL views for main cluster
    |       |-- System catalog tables
    |
    |-- Redshift data api (lets you pass an sql query to redshift cluster and execute query on cluster and sends the results back in a json paginated format)
    |
    |-- DATASHARE (Redshift sql DDL command used to share read-only data between clusters/workgroups/accounts)
    |       producer (2342544656)
    |       ``` 
    |           CREATE DATASHARE mgc_datashare;
    |
    |           ALTER DATASHARE mgc_datashare
    |           ADD TABLE admin.maintable;
    |
    |           ALTER DATASHARE mgc_datashare
    |           ADD ACCOUNT '1323534524';
    |       ```
    |
    |       consumer (1323534524)
    |       ```
    |           CREATE DATABASE my_db_from_mgc
    |           FROM DATASHARE mgc_datashare
    |           OF ACCOUNT '2342544656'
    |           REGION 'ap-south-1';
    |
    |-- WORKLOAD MANAGEMENT
    |       |
    |       |-- SQA Short Query Acceleration (tasks such as CREATE TABLE, SELECT statements come under this --- will be prioritiezed as the queries will finish quicker)
    |       |-- Auto workload Management (Allows queries to be ran based on Memory availablity --- performs parallel processing if memory utlization is low)
    |       |-- Manual Workload Management (Manually set query group routes)
    |           ```
    |               SET query_group TO 'etl';
    |               actual query you are tyring to execute
    |           ```
    |
    |-- MachineLearning (Creates ML functions using sql query)
    |
    |-- Access Control in RedShift
            ```
                CREATE USER user_123 PASSWORD 'pass_123';
                CREATE USER superuser_123 PASSWORD 'pass_123' SUPERUSER;
                CREATE GROUP dev123;
                ALTER GROUP dev123 ADD USER user_123;
                GRANT SELECT ON ALL TABLES IN SCHEMA sch123 TO GROUP dev123;
                CREATE ROLE new_dev123; -- modern replacement for groups
                GRANT ROLE new_dev123 TO USER user_123;

                -- Fine Grained Access control
                GRANT SELECT (column1, column2) ON table123 TO user_123;

                ---------- Row Level policy ----------
                CREATE RLS POLICY rls_123
                WITH (policynumber INT)
                USING (
                    policynumber IN 
                        (select policy number from mytable where groupname = "currentproject")
                );

                ALTER TABLE mytable
                ADD RLS POLICY rls_123;

                --------- Masking Policy --------
                CREATE MASKING POLICY mask_email_123
                WITH (email VARCHAR(256))
                USING ('*****'::TEXT);

                ATTACH MASKING POLICY
                ON mytable(email)
                TO ROLE new_dev_123
                PRIORITY 20;
            ```



Redshift Serverless (seperate service than normal RedShift, can only join with tables from RedShift using DATASHARE functionality)
    |
    |-- Storage {Uses S3 as underlying storage, no need to load into warehouse again}