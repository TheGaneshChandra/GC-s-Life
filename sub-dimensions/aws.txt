# AWS
> Important Links
- https://aws.amazon.com/documentation-overview/ {High level overview of service features}
- https://docs.aws.amazon.com/cli/latest/ {Command line documentation}

## Glue
> https://github.com/TheGaneshChandra/awsglue

## Kinesis

### Kinesis Data stream
> Near real time bits of data transfer from PRODUCER to AWS which a PRODUCER can consume
- Streams
    |
    |-- Shards <!-- Different threads based on partition key of data in a stream -->
        |
        |-- Producer <!-- Any device or application that sends data to Kinesis data streams -->
        |   |
        |   |-- Send One record per call <kinesis.put_record( StreamName="", Data="", PartitionKey="")>
        |   |-- Send Batch upto 500 <kinesis.put_records( StreamName="", Records=[ {"Data":, "PartitionKey":} ])>
        |
        |-- SequenceNumber <!-- A unique number that gets incremented for each record in shard -->
        |
        |-- PartitionKey <!-- A column used to define which shard the data goes to -->
        |   |
        |   |-- MD5_Hashing <!-- A type of hash that gives values in range 0-2^128, Used to differentiate unique partition keys -->
        |
        |-- ShardIterator <!-- A pointer from where the shard data is read forward -->
        |   |
        |   |-- "TRIM HORIZON" <!-- Starts from the oldes record -->
        |   |-- "LATEST" <!-- Starts from the latest -->
        |   |-- "AT_SEQUENCE_NUMBER" <!-- Starts from a given sequence number -->
        |   |-- "AFTER_SEQUENCE_NUMBER" <!-- Starts from the next record of given seq number -->
        |   |-- "AT_TIMESTAMP" <!-- Starts from the given timestamp-->
        |
        |-- Retention_Period <!-- Amount of time Data is alive in data stream before getting deleted (default: 24hours, max: 365days) -->
        |
        |-- Checkpointing <!-- Saves the last sequenceNumber so to process only the next records handled by user or using KCL -->
        |
        |-- Resharding <!-- Splitting or Merging of Shards based on Throughput limit (write: 1MB/sec, 1000 records/sec) or (read: 2 MB/sec, 5 Get_record() calls -->
        |
        |-- Provisioned_Mode <!-- AWS handling the Automatic scaling (Resharding) of shards based on data throughput-->
        |
        |-- Consumers <!-- Any Application that reads data from Kinesis data streams -->
        |   |
        |   |-- Classic <GetRecords(shard_iterator)> <!-- Total of 2MB/sec Bandwidth for all consumers with around 200ms latency -->
        |   |-- Enhanced_Fan_Out (EFO) <SubscribeToShard()> <!-- Each consumer gets 2MB/sec bandwidth with 70ms latency -->
        |
        |-- Kinesis_Client_Library (KCL) <!-- A library that handles some tasks related to Kinsis -->
            |
            |-- Checkpointing <checkpoint(sequence_number)> <!-- Saves the last sequenceNumber so to process only the next records (stored in dynamodb) -->
            |-- Shard_Discovery
            |-- LoadBalancing
            |-- Fault recovery
            |-- ExcatlyOnceProcessing

### Kinesis FireHose
> Fully managed streaming service that **delivers** data to S3, RedShift, OpenSearch, Splunk, Snowflake
- Delivery Stream <!-- The stream that moves the data from producer to the consumer destination -->
    ```python
        
        import boto3
        firehose = boto3.client("firehose")
        firehose.create_delivery_stream(
            DeliveryStreamName="",
            S3DestinationConfiguration = {}
        )

        # send record
        firehose.put_record(
            DeliveryStreamName = "",
            Record = [{}]
        )
    ```
    |
    |-- Buffering <!-- Firehose delivers data in a batch format, so if buffers the stream data based on chosen size(1MB-128MB), interval(1-900sec) -->
    |
    |-- Data Transformtion using Lambda <!-- Lambda function to be executed on each batch before putting into Consumer destination -->
    |
    |-- Compression Formats <!-- Can Compress before writing to destination (GZIP, Snappy, ZIP) -->
    |
    |-- Data Formats <!-- Can convert the data to desired format (JSON, PARQUET, ORC) before writing  -->
    |
    |-- S3 backup mode <!-- if writing to destination fails, will save the data in this backup bucket -->
    |
    |-- Auto Retires
    |
    |-- AutoScaling

### AWS Managed Apache Flink
> A distributed processing engine for stateful computations of bounded and unbounded data
- Stateful Stream processing <!-- Remembers past events, Keep local state on each operator checkpointed to S3/Dynamodb-->
- Event-Time processing <!-- Processes events when happened, not when arrived -->
- Exaclty once Stateful processing Guarenteed <!-- Industry standard for exactly once processing -->
- Windowing functionaliy <!-- events grouped by time or count -->
- Fault Tolerance, Savepointing <!-- Can restart entire pipeline without losing state -->
- Batch + Streaming

### AWS Managed Kafka
> Distributed log-based Event streaming platform used for messaging, **event storage**, and streaming analytics
- Durable storage layer <!-- Can store data upto years, replay history at any time -->
- Stream Replay (Time travel) <!-- Consume old data at any offset by any consumer -->
- Independent Consumers <!-- Consumers can access, replay data independently -->
- Extremely High-throughput <!-- Millions per second -->
- Kafka Connect <!-- can be connected to many connectors with no code -->

## s3

- Buckets
    |
    |-- MetaData tables <!-- Iceberg tables containing Data about the objects in the bucket that can be queryed in Athena-->
    |       |
    |       |-- Journal Metadata <!-- Default Metadata table, records events occured on objects inside bucket -->
    |       |       |
    |       |       |-- SystemDefined <!-- Created automatically by aws, can also manually specify when uploading a new object -->
    |       |       |-- UserDefined <!-- Custom key value pairs given by user -->
    |       |
    |       |-- Inventory Metadata <!-- Metadata table containing list of all objects and version present in the bucket -->
    |
    |-- Properties
    |       |
    |       |-- BucketVersioning <!-- Keeps Versions of objects instead of deleting them permanently -->
    |       |-- MFA delete <!-- Needs an MFA key to delete--- This can only be enabled using CLI, SDK -->
    |       |
    |       |-- Bucket AttributeBasedAccessControl ABAC <!-- Uses user defined tags to grant IAM permissions -->
    |       |
    |       |-- Tags
    |       |
    |       |-- Encryption
    |       |       |
    |       |       |-- Server side Encryption with S3 Managed Keys
    |       |       |-- Server side Encryption with KMS
    |       |       |-- Dual Layer Server side Encryption with KMS
    |       |       |-- Server side Encryption with Customer Provided Keys
    |       |       |
    |       |       |-- S3 Bucket Keys <!-- Uses one temporary key for Bucketlevel Encryption, which is used to create dataKey for each object without reaching out to KMS all the time -->
    |       |
    |       |-- S3_Intelligent_Tiering <!-- If data access is not predicatable then S3_Intelligent_Tiering--- If data access is predictable tjen s3 LifeCycle Rules -->
    |       |
    |       |-- S3 Server Access Loggging <!-- Logs requests for server access -->
    |       |
    |       |-- s3_Event_Notifications <!-- Create an Event Notification that notifies when that event occurs, used to trigger other stuff -->
    |       |
    |       |-- s3_TransferAccelertion 
    |       |
    |       |-- s3_Object_Lock
    |       |       |
    |       |       |-- Object Lock Legal Hold <!-- Will not allow the object deletion until the object lock is removed by anyone -->
    |       |       |-- Object Lock Retention <!-- Not even root user can delete the object until Retention period expires -->
    |       |
    |       |-- Requestor_pays
    |       |
    |       |-- Static Website Hosting
    |
    |-- Management
    |       |
    |       |-- LifeCycle configuration <!-- deletes / Moves the selected object versions between storage classes based on given condition -->
    |       |
    |       |-- Replication rules <!-- Replicates the selected contents from source bucket to destination bucket -->
    |       |       |
    |       |       |-- Replication Time Control <!-- Replicates within 15 mins -->
    |       |       |-- Delete Marker Replication <!-- Replicates the delete marker also, this is disabled by default -->
    |       |       |-- Replica Modification Sync <!-- Replicates the metadata changes too -->
    |       |
    |       |-- Inventory configuration <!-- Creates reports with list of objects and their metadata -->
    |
    |-- Access Points
    
- Objects
    |
    |-- Checksums <!-- used to check if the data before uploading and after uploading are same using hashing algorithms -->


## DynamoDB
```
# in all dynamodb function calls, we can add these expressions -- these are unique and important
- ConditionalExpression
- UpdateExpression
- ExpressionAttributeNames
- ExpressionAttributeValues
- ProjectionExpression
```
- Tables
    |
    |-- PartitionKey <!-- Primary key for DynamoDB, stores values in this column as hash and used for quick retreival -->
    |-- SortKey <!-- If a sort key is provided, then the PartitionKey & SortKey together create a composite Primary key -->
    |
    |-- Secondary Index
    |       |
    |       |-- Local Secondary Index (LSI) {Creates an Index based on table's parititionKey and new sortkey--- used to query based on this new sortkey}
    |       |-- Global Secondary Index (GSI) {Creates an Index based on given parititionKey and sortKey}
    |       |
    |       |-- AttributeProjections {Saves extra Attributes/columns along with PartitionKey, SortKey in the indices}
    |
    |-- Table Class
    |       |
    |       |-- DynamoDB Standard
    |       |-- DynamoDB Standard Infrequent-Access (Standard IA)
    |
    |-- Capacity
    |       |
    |       |-- On-demand (we have an option for maximum table throuput to limit)
    |       |-- Provisioned
    |                |
    |                |-- AutoScaling (1-anyupper limit)
    |                |
    |                |-- RCU (Read Capacity Units) {1 RCU: 4kb/sec reading (eventually consistent - not so latest results)--- 1 RCU: 2kb/sec reading (strongly consistent - latest results)--- 1RCU: 1kb/sec reading (transactional)}
    |                |-- WCU (Write Capacity Units) {1 WCU: 1kb/sec (standard)--- 1 WCU: 0.5kb/sec (transactional)}
    |
    |-- DynamoDB Streams {provides the Change Data Capture (CDC) to the users in form of streams -- Can also be used as trigger for Lambda functions -- similar to kinesis data streams but most stuff managed by AWS}
    |
    |-- Warm throuput (Increases the capcacity before peak events)
    |
    |-- Deletion Protection (Protects the table from getting deleted/dropped)
    |
    |-- TTL Time to Live (Automatically deletes items after given time expires)
    |
    |-- Global Tables (Creates a duplicate table in another region -- uses DynamoDB Streams in the backend)
    |
    |-- Point-in-time-recovery (PITR) {Used to take continous backups upto 35days -- Can be restored with specified data backup}


## Redshift
        |
        |-- Clusters {Collection of nodes}
        |       |
        |       |-- Nodes {Consider like a server}
        |       |       |
        |       |       |-- Types
        |       |       |       |
        |       |       |       |-- RA3 Redshift Analytics 3 {New--- Uses RMS--- Decople Compute, Storage--- Multi-AZ as it can just get data from s3}
        |       |       |       |
        |       |       |       |-- DC2 Dense Compute 2 {Old--- Uses local SSD--- Sinlge AZ--- Compute, Storage are hardware inside the node--- Recommended for data < 1TB}
        |       |       |
        |       |       |-- Architechture
        |       |               |
        |       |               |-- Leader Node --> [Compute Node, Compute Node,...]
        |       |               |
        |       |               |-- Node Slices {A Core of the Node's CPU, and the RAM, other stuff also gets splitted}
        |       |               |
        |       |               |-- CPU {Here is where the computataion happens, using the L1 cache, RAM, to perform joins etc}
        |       |               |-- RAM {Used for quick data read}
        |       |               |-- SSD storage {Used for data storage and used in cases when RAM is insufficient}
        |       |               |-- NetworkFabric {The Network layer that moves the data to and from outside}
        |       |
        |       |-- SnapShots {Can instantly copy the entire RMS Data, Metadata to recreate the cluster and store in s3} <Need to know Snapshots != backups>
        |       |       |
        |       |       |-- Incremental Snapshots {Captures only changed blocks in RMS}
        |       |       |-- Automated Snapshots {1 day retention (default) --- Default - Sanpshots taken every 8 hours / 5GBb of data change}
        |       |       |-- Manual Snapshots {Infinite retention by default}
        |       |
        |       |-- Resizing
        |       |       |
        |       |       |-- Elastic Resize {Fast --- Change number of nodes without restart --- Cant change node types --- Can only do minor changes --- Nodes have to be same family}
        |       |       |
        |       |       |-- Classic Resize {Slow --- Change number of nodes, node types too --- Can change node types --- Can do drastic node changes --- Nodes can be any family}
        |       |
        |       |-- Distribution {Distributing / Partitioning Data accross nodes}
        |               |
        |               |-- ALL Distribution {Distribute entire table accross nodes}
        |               |-- KEY Distribution {Distribute based on column}
        |               |-- EVEN Distribution {Distribute data evenly based on rows}
        |               |-- AUTO Distribtion {Redshift chooses the Distribution style in the order (ALL -> KEY -> EVEN)}
        |
        |        
        |-- RMS Redshift Managed Storage {Data for the warehouse is stored --- Stores Block Level data in S3 --- Only used in RA3 nodes --- On the backend its either S3 for cold data or SSD of the node for hot data}
        |       |
        |       |-- Sharing data accross AWS Regions {Works only with RA3 --- Wont copy data, just sends the related data to other regions SSD from the current region's S3}
        |
        |-- `VACCUM [type_of_vaccum] [[table_name | TO threshold_pct] [BOOST]]` (Used to reclaim space by resorting the table on which detles and updates might have occuerd)
        |       |
        |       |-- FULL (Sorts and reclaims space)
        |       |-- SORT ONLY (Just sorts the table but wont reclaim any space)
        |       |-- DELETE ONLY (Recalim disk space only, wont sort anything)
        |       |-- REINDEX (Rebuilds the index of the tables)
        |       |-- RECLUSTER (Same as FULL Vaccum, but better)
        |
        |-- `COPY table_name from 'location' IAM_ROLE 'iamrole'`  (Used to load large amound of data to redshift --- uses Redshift MPP --- support compression --- uses manifest file for s3 files)
        |
        |-- `INSERT INTO table select * from table_name` (Inserts data into redshift from catalog table or Redshift spectrum --- Much slower than copy)
        |
        |-- `UNLOAD (query) TO 's3 uri ending with _' IAM_ROLE 'iamrole' FORMAT AS PARQUET` (Inserts data into s3 from Redshift based on query)
        |
        |-- `ANALYSE table_name` (Used to decide if we need to vaccum the table)


## Redshift Serverless
        |
        |-- Storage {Uses S3 as underlying storage, no need to load into warehouse again}